diff --git a/src/no_asm.h b/src/no_asm.h
index d2ea50f..eb736b8 100644
--- a/src/no_asm.h
+++ b/src/no_asm.h
@@ -8,6 +8,10 @@
 typedef unsigned long long llimb_t;
 #endif
 
+#if LIMB_T_BITS==64
+typedef unsigned __int128 llimb_t;
+#endif
+
 #if defined(__clang__)
 # pragma GCC diagnostic ignored "-Wstatic-in-inline"
 #endif
@@ -83,7 +87,48 @@ inline void sqr_mont_##bits(vec##bits ret, const vec##bits a, \
 MUL_MONT_IMPL(256)
 #undef mul_mont_256
 #undef sqr_mont_256
+
+#if USE_MUL_MONT_384_ASM
+// will be implemented in asm
+#if USE_RVV
+void blst_mul_mont_384_rvv(limb_t ret[], const limb_t a[], const limb_t b[],
+                           const limb_t n[], const limb_t np1[], const uint64_t size);
+inline void mul_mont_384(vec384 ret, const vec384 a,
+                  const vec384 b, const vec384 p, limb_t n0) {
+    const limb_t A[8] = {a[0], a[1], a[2], a[3], a[4], a[5], 0, 0};
+    const limb_t B[8] = {b[0], b[1], b[2], b[3], b[4], b[5], 0, 0};
+    const limb_t n[8] = {0xb9feffffffffaaab, 0x1eabfffeb153ffff, 0x6730d2a0f6b0f624, 0x64774b84f38512bf, 0x4b1ba7b6434bacd7, 0x1a0111ea397fe69a, 0, 0};
+    const limb_t n_reverse[8] = {0x89f3fffcfffcfffd, 0x286adb92d9d113e8, 0x16ef2ef0c8e30b48, 0x19ecca0e8eb2db4c, 0x68b316fee268cf58, 0xceb06106feaafc94, 0, 0};
+    limb_t new_ret[8] = {};
+    blst_mul_mont_384_rvv(new_ret, A, B, n, n_reverse, 1);
+    ret[0] = new_ret[0];
+    ret[1] = new_ret[1];
+    ret[2] = new_ret[2];
+    ret[3] = new_ret[3];
+    ret[4] = new_ret[4];
+    ret[5] = new_ret[5];
+}
+inline void sqr_mont_384(vec384 ret, const vec384 a,
+                            const vec384 p, limb_t n0)
+{   mul_mont_n(ret, a, a, p, n0, NLIMBS(384));   }
+#else
+void blst_mul_mont_384(vec384 ret, const vec384 a,
+                  const vec384 b, const vec384 p, limb_t n0);
+
+inline void mul_mont_384(vec384 ret, const vec384 a,
+                  const vec384 b, const vec384 p, limb_t n0) {
+  blst_mul_mont_384(ret, a, b, p, n0);
+  return;
+}
+
+inline void sqr_mont_384(vec384 ret, const vec384 a,
+                            const vec384 p, limb_t n0)
+{   mul_mont_n(ret, a, a, p, n0, NLIMBS(384));   }
+#endif
+#else
 MUL_MONT_IMPL(384)
+#endif
+
 
 static void add_mod_n(limb_t ret[], const limb_t a[], const limb_t b[],
                       const limb_t p[], size_t n)
@@ -582,6 +627,17 @@ inline limb_t sgn0_pty_mont_384x(const vec384x a, const vec384 p, limb_t n0)
     return sgn0_pty_mod_384x(tmp, p);
 }
 
+
+#ifdef USE_MUL_MONT_384_ASM
+void blst_mul_mont_384x(vec384x ret, const vec384x a, const vec384x b,
+                          const vec384 p, limb_t n0);
+
+inline void mul_mont_384x(vec384x ret, const vec384x a, const vec384x b,
+                          const vec384 p, limb_t n0) {
+  return blst_mul_mont_384x(ret, a, b, p, n0);
+}
+
+#else
 void mul_mont_384x(vec384x ret, const vec384x a, const vec384x b,
                           const vec384 p, limb_t n0)
 {
@@ -596,6 +652,8 @@ void mul_mont_384x(vec384x ret, const vec384x a, const vec384x b,
     sub_mod_n(ret[1], bb, aa, p, NLIMBS(384));
     sub_mod_n(ret[1], ret[1], cc, p, NLIMBS(384));
 }
+#endif
+
 
 /*
  * mul_mont_n without final conditional subtraction, which implies
diff --git a/src/vect.h b/src/vect.h
index bef15cf..f3e31eb 100644
--- a/src/vect.h
+++ b/src/vect.h
@@ -7,7 +7,10 @@
 #define __BLS12_381_ASM_VECT_H__
 
 #include <stddef.h>
+#include <stdint.h>
 
+
+#if 0
 #if defined(__x86_64__) || defined(__aarch64__)
 /* These are available even in ILP32 flavours, but even then they are
  * capable of performing 64-bit operations as efficiently as in *P64. */
@@ -35,6 +38,17 @@ typedef unsigned long limb_t;
 #  endif
 #endif
 
+#endif
+
+#define __BLST_NO_ASM__
+#if 0
+#define LIMB_T_BITS   32
+typedef uint32_t limb_t;
+#else
+#define LIMB_T_BITS   64
+typedef uint64_t limb_t;
+#endif
+
 /*
  * Why isn't LIMB_T_BITS defined as 8*sizeof(limb_t)? Because pre-processor
  * knows nothing about sizeof(anything)...
